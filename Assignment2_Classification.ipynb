{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">Lukas</p>| <p style=\"text-align: left\">Kurz</p> | 12007739 |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 344.063: Special Topics - Natural Language Processing with Deep Learning (SS2022)\n",
    "\n",
    "# Assignment 2: Document Classification with Attention and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Author:** Navid Rekab-saz<br>\n",
    "**Email:** navid.rekabsaz@jku.at<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-classification-attention\"><li style=\"font-size:large;font-weight:bold\">Task A: Document Classification with Attention (10 points)</li></a>\n",
    "    <a href=\"#section-classification-transformer\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with Transformer (10 points)</li></a>\n",
    "    <a href=\"#section-classification-bert\"><li style=\"font-size:large;font-weight:bold\">Task C: Document Classification with BERT (5 points)</li></a>\n",
    "    <a href=\"#section-interpretation\"><li style=\"font-size:large;font-weight:bold\">Task D: Interpreting Attention Weights (3 extra points)</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment objective\n",
    "\n",
    "The aim of this assignment is to implement a document (sequence) classification model with PyTorch using Attention networks, Transformers, and BERT. You can/should use the codes implemented in the previous assignment. Main aspects of the deep learning models in this assignment such as preprocessing, dictionary, word embeddings, data batching, loss functions, early stopping, and evaluation are explained in the previous assignment, and **are expected to be fully correct and functional in this assignment**.\n",
    "\n",
    "The assignment has **25 points** in total as well as **3 overall extra points**. This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contains code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7) and `PyTorch` (>1.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `transformer`, `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, two datasets are provided. The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "The difference between the datasets is in their sizes. We refer to these as `medium` and `small`, containing an overall number of 38,000 and 12,000 annotated text excerpts, respectively. Select one of the datasets, and use it for all of the tasks. `medium` provides more data and therefore reflects a more realistic scenario. `small` is however provided for the sake of convenience, particularly if running the experiments on your available hardware takes too long. Using `medium` is generally recommended, but from the point of view of assignment grading, there is no difference between the datasets.\n",
    "\n",
    "Download the dataset from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f07f0df27c017f8ea132693d61/38160/1583790728782872458/nlpwdl2022_data.zip).\n",
    "\n",
    "Whether `medium` or `small`, you will find the following files in the provided zip file:\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing Experiments Results\n",
    "\n",
    "It is encouraged that you log and store any information about the training and evaluation of the models in an ML dashboard like [`TensorBoard`](https://www.tensorflow.org/tensorboard) or [`wandb`](https://wandb.ai/site). This can contain any important aspect of training such as the changes in the evaluation results on validation, training loss, or learning rate. \n",
    "\n",
    "To this end, in the case of `TensorBoard`, after finalizing all experiments and cleaning any unnecessary experiment, publish the log files results through [`TensorBoard.dev`](https://tensorboard.dev). A simple way of doing it is by running the following command in the folder of log files:\n",
    "\n",
    "`tensorboard dev upload --name my_exp --logdir path/to/output_dir`\n",
    "\n",
    "`TensorBoard.dev` uploads the necessary files and provides a URL to see the TensorBoard's console. Insert the URL in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL :** *EDIT!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Input processing, libraries, etc. from previous assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "# for preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# for data batching\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for model\n",
    "from torch.nn import Embedding, LSTM, TransformerDecoder, TransformerDecoderLayer\n",
    "from copy import deepcopy\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukaskurz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lukaskurz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lukaskurz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the models and data from nltk for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the word2vec model\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "result_path = './results/'\n",
    "writer = SummaryWriter(log_dir=os.path.join(result_path, 'tensorboard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Input processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, Dictionary, and Word Embedding Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset\n",
    "\n",
    "We read the train, test and validation splits, along with the labels.\n",
    "The datasets are returned as pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset_base_path = './data/', dataset_size = 'medium'):\n",
    "    \"\"\"\n",
    "    Read the dataset from the given path.\n",
    "    :param dataset_base_path: the base path of the dataset\n",
    "    :param dataset_size: the size of the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    training_dataset_path = os.path.join(dataset_base_path, 'thedeep.{}.train.txt'.format(dataset_size))\n",
    "    validation_dataset_path = os.path.join(dataset_base_path, 'thedeep.{}.validation.txt'.format(dataset_size))\n",
    "    test_dataset_path = os.path.join(dataset_base_path, 'thedeep.{}.test.txt'.format(dataset_size))\n",
    "    label_dataset_path = os.path.join(dataset_base_path, 'thedeep.labels.txt'.format(dataset_size))\n",
    "\n",
    "    training_df = pd.read_csv(training_dataset_path, names=[\"sentence_id\", \"text\", \"label\"])\n",
    "    validation_df = pd.read_csv(validation_dataset_path, names=[\"sentence_id\", \"text\", \"label\"])\n",
    "    test_df = pd.read_csv(test_dataset_path, names=[\"sentence_id\", \"text\", \"label\"])\n",
    "    label_df = pd.read_csv(label_dataset_path, names=[\"label\"])\n",
    "\n",
    "    return training_df, validation_df, test_df, label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11609</td>\n",
       "      <td>• 214,000 students affected as schools close d...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28291</td>\n",
       "      <td>The primary reported needs for IDPs across the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9695</td>\n",
       "      <td>Some 602 000 IDPs are now spread across the co...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7781</td>\n",
       "      <td>South Sudanese soldiers accused of raping at l...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31382</td>\n",
       "      <td>Since the beginning of 2017, 18 882 suspected/...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                               text  label\n",
       "0        11609  • 214,000 students affected as schools close d...      9\n",
       "1        28291  The primary reported needs for IDPs across the...      4\n",
       "2         9695  Some 602 000 IDPs are now spread across the co...      3\n",
       "3         7781  South Sudanese soldiers accused of raping at l...      9\n",
       "4        31382  Since the beginning of 2017, 18 882 suspected/...     11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df, validation_df, test_df, label_df = read_dataset()\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and tokenizing\n",
    "\n",
    "We use the `+foobar+` notation to replace certain words, such as dates and numbers. The `+` sign is compatible with the Lemmatizer from nltk, which is why it was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dates(s: str):\n",
    "    \"\"\"\n",
    "    Replace dates with a special token.\n",
    "    \"\"\"\n",
    "    s = re.sub(r'\\d{1,2}[\\.\\,\\|\\-\\_\\/\\\\]\\d{1,2}[\\.\\,\\|\\-\\_\\/\\\\]\\d{2,4}', ' +date+ ', s) \n",
    "    s = re.sub(r'\\d{2,4}[\\.\\,\\|\\-\\_\\/\\\\]\\d{1,2}[\\.\\,\\|\\-\\_\\/\\\\]\\d{1.2}', ' +date+ ', s)\n",
    "    s = re.sub(r'[1-2]\\d{3}', ' +year+ ', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def preprocess(s: str):\n",
    "    \"\"\"\n",
    "    Preprocess the given string.\n",
    "    \"\"\"\n",
    "    s = replace_dates(s)\n",
    "    \n",
    "    s = re.sub(\"[+-]?([0-9]*[.,])?[0-9]+\", \" +num+ \", s)  # escape integers and floats\n",
    "    s = re.sub('[^a-zA-Z\\d\\s+]', \"\", s) # remove non alphanumerics, except for escape char\n",
    "    s = re.sub('\\b[\\w]{1}\\b', \"\", s) # remove 1 length words\n",
    "    s = re.sub('(?<![num|year|date])\\+(?!num|year|date\\+)', '', s) # match alone standing + signs\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def tokenize(article: str):\n",
    "    \"\"\"\n",
    "    Tokenize the given string.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in word_tokenize(article) if len(token) > 1 and not token in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, article):\n",
    "        return [self.wnl.lemmatize(t) for t in tokenize(preprocess(article))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: • 214,000 students affected as schools close due to insecurity • 65 people killed already in 2018 by improvised explosives• Mass grave uncovered following military violations\n",
      "\n",
      "dates replaced: • 214,000 students affected as schools close due to insecurity • 65 people killed already in  +year+  by improvised explosives• Mass grave uncovered following military violations\n",
      "\n",
      "preprocessed text:   +num+  students affected as schools close due to insecurity   +num+  people killed already in  +year+  by improvised explosives mass grave uncovered following military violations\n",
      "\n",
      "tokenized text: +num+ student affected school close due insecurity +num+ people killed already +year+ improvised explosive mass grave uncovered following military violation\n"
     ]
    }
   ],
   "source": [
    "def show_tokenizing_steps(demo_text: str):\n",
    "    lemma_tokenizer = LemmaTokenizer()\n",
    "\n",
    "    print('original text: {}\\n'.format(demo_text))\n",
    "    dates_replaces_text = replace_dates(demo_text)\n",
    "    print('dates replaced: {}\\n'.format(dates_replaces_text))\n",
    "    preprocessed_text = preprocess(demo_text)\n",
    "    print('preprocessed text: {}\\n'.format(preprocessed_text))\n",
    "    tokenized_text = (' ').join(lemma_tokenizer(preprocessed_text))\n",
    "    print('tokenized text: {}'.format(tokenized_text))\n",
    "\n",
    "show_tokenizing_steps(demo_text = training_df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and reduce dictionary\n",
    "\n",
    "We create a dictionary from all the feature names in the vectorizer and then use a cut-off threshold to reduce the dictionary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(cut_off_threshold = 0.0001):\n",
    "    \"\"\"\n",
    "    Get the dictionary of the dataset.\n",
    "\n",
    "    return tuple of (dictionary, reduced_dictionary)\n",
    "    :param cut_off_threshold: the threshold of the word frequency\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=LemmaTokenizer())\n",
    "    training_vectorized = vectorizer.fit_transform(training_df['text'])\n",
    "\n",
    "    word_list = vectorizer.get_feature_names_out()\n",
    "    count_list = training_vectorized.toarray().sum(axis=0)\n",
    "    token_dictionary = dict(zip(word_list,count_list))\n",
    "\n",
    "    word_amount = sum(token_dictionary.values())\n",
    "    reduced_token_dictionary = {}\n",
    "    for word in token_dictionary:\n",
    "        if token_dictionary[word] > word_amount*cut_off_threshold:\n",
    "            reduced_token_dictionary[word] = token_dictionary[word]\n",
    "    \n",
    "    return token_dictionary, reduced_token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is 36216\n",
      "+date+ +num+ +year+ aa aaf aah aal ...zuwara zuwarah zuwaras zuwayed zvulun zwak zwara\n",
      "The length of the reduced dictionary is 1530\n",
      "+date+ +num+ +year+ ability able aboveaverage absence ...yet yield yobe young youth zambia zone\n"
     ]
    }
   ],
   "source": [
    "def show_dictionaries():\n",
    "    \"\"\"\n",
    "    Show the length of the dictionaries, before and after cutoff.\n",
    "    Might take some seconds to compute.\n",
    "    \"\"\"\n",
    "    full_token_dictionary, reduced_token_dictionary = get_dictionary()\n",
    "    print('The length of the dictionary is {}'.format(len(full_token_dictionary)))\n",
    "    print(' '.join(list(full_token_dictionary.keys())[:7]) + ' ...' + ' '.join(list(full_token_dictionary.keys())[-7:]))\n",
    "    print('The length of the reduced dictionary is {}'.format(len(reduced_token_dictionary)))\n",
    "    print(' '.join(list(reduced_token_dictionary.keys())[:7]) + ' ...' + ' '.join(list(reduced_token_dictionary.keys())[-7:]))\n",
    "\n",
    "    return full_token_dictionary, reduced_token_dictionary\n",
    "\n",
    "_, token_dictionary = show_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map word embeddings to dictionary words\n",
    "\n",
    "We map the words of the dictionary to their respective word embeddings from `word2vec`.\n",
    "\n",
    "Out-of-vocabulary tokens are replaced with a random vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(dictionary: dict):\n",
    "    \"\"\"\n",
    "    Convert the word embedding dict to a matrix and return a torch.Embedding\n",
    "    :param dictionary: the embedding dictionary\n",
    "    \"\"\"\n",
    "    mean = np.mean(word2vec.vectors)\n",
    "    std = np.std(word2vec.vectors)\n",
    "    dictionary_keys = list(dictionary.keys())\n",
    "    dictionary_keys.insert(0, '+pad+')\n",
    "    dictionary_keys.insert(1, '+oov+')\n",
    "    np.random.seed(42069)\n",
    "    word_lookup = np.zeros(shape=(len(dictionary_keys), word2vec.vector_size))\n",
    "    for idx, word in enumerate(dictionary_keys):\n",
    "        if word in word2vec:\n",
    "            word_lookup[idx] = word2vec[word]\n",
    "        else:\n",
    "            word_lookup[idx] = np.random.normal(loc=mean, scale=std, size=word2vec.vector_size)\n",
    "\n",
    "    return Embedding.from_pretrained(torch.tensor(word_lookup, dtype=torch.float32), freeze=False, padding_idx=0), dictionary_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary_keys: +pad+, +oov+, +date+, +num+, +year+, ability, able, aboveaverage, absence, abu...\n",
      "embedding_matrix shape: torch.Size([1532, 300])\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, dictionary_keys = get_embedding(token_dictionary)\n",
    "print('dictionary_keys: {}...'.format(', '.join(dictionary_keys[0:10])))\n",
    "print('embedding_matrix shape: {}'.format(embedding_matrix.weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batching and Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dict_ids(words: list, dictionary_keys: list):\n",
    "    \"\"\"\n",
    "    Maps a list of words to their respective indexes/ids in the dictionary.\n",
    "    Out of vocabulary words are skipped/ignored.\n",
    "    \n",
    "    :param words: list of strings\n",
    "    :param dictionary: dictionary of keys\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # using a try-catch to account for words not in dictionary is much faster than checking each word with an if\n",
    "    # since most of the words are found and only a small percentage throws an exception, that needs to be caught\n",
    "    for word in words:\n",
    "        try:\n",
    "            results.append(dictionary_keys.index(word))\n",
    "        except:\n",
    "            results.append(dictionary_keys.index('+oov+'))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def transform_document(document: str, dictionary_keys: list, tokenizer: LemmaTokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    Transform the document to a list of indexes.\n",
    "    Document is preprocessed and tokenized.\n",
    "    Then it is either cut or padded to max_length.\n",
    "    Padding is done with the -1 value, since that matches no token id.\n",
    "    :param document: the document to be transformed\n",
    "    :param dictionary_keys: the dictionary keys\n",
    "    :param tokenizer: the tokenizer\n",
    "    :param max_length: the max length of the document\n",
    "    \"\"\"\n",
    "    words = tokenizer(document)\n",
    "    ids = map_dict_ids(words, dictionary_keys)\n",
    "    cutoff_ids = ids[0:max_length]\n",
    "    padded_ids = np.pad(cutoff_ids, (0,max_length-len(cutoff_ids)), mode='constant', constant_values=0) # is pad index\n",
    "    \n",
    "    return padded_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the creation of the documents, we utilize multi threading. With small dictionary, this is not really relevant, but processing time increased when a bigger dictionary ~ smaller cut-off is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_document_mp(args):\n",
    "    \"\"\"\n",
    "    Wrapper for transform_document, that accepts the args as a tuple.\n",
    "    \"\"\"\n",
    "    return [transform_document(d, args[2], args[3], args[4]) for d in args[0]], args[1]\n",
    "\n",
    "def create_arguments(documents: list, labels: list, batch_size: int, dictionary_keys: list, tokenizer: LemmaTokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    Create the arguments list for the multiprocessing.\n",
    "    :param documents: the documents to be transformed\n",
    "    :param labels: the labels of the documents\n",
    "    :param batch_size: the batch size used for the multiprocessing jobs.\n",
    "    :param dictionary_keys: the dictionary keys\n",
    "    :param tokenizer: the tokenizer\n",
    "    :param max_length: the max length of the output document\n",
    "    \"\"\"\n",
    "    arguments = []\n",
    "    n = len(documents)\n",
    "    start = 0\n",
    "    for end in range(batch_size, n, batch_size):\n",
    "        arguments_batch = (documents[start:end],labels[start:end], dictionary_keys, tokenizer, max_length)\n",
    "        arguments.append(arguments_batch)\n",
    "        start = end\n",
    "    # if n % batch_size != 0:\n",
    "    arguments.append((documents[start:],labels[start:], dictionary_keys, tokenizer, max_length))\n",
    "\n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap all of the above code into a pytorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, max_document_length: int, tokenizer: LemmaTokenizer, dictionary_keys: list, n_jobs = 10, loading_label = 'Transforming Documents'):\n",
    "        \"\"\"\n",
    "        Create a dataset from a pandas dataframe.\n",
    "\n",
    "        Throws error if n_jobs is bigger than available cpu core count\n",
    "        :param df: the pandas dataframe\n",
    "        :param max_document_length: the max length of the documents\n",
    "        :param tokenizer: the tokenizer\n",
    "        :param dictionary_keys: the dictionary keys\n",
    "        :param n_jobs: the number of jobs for the multiprocessing\n",
    "        \"\"\"\n",
    "        if n_jobs > cpu_count():\n",
    "            raise ValueError('n_jobs must be less than or equal to the number of available CPU cores')        \n",
    "\n",
    "        transformed_documents = []\n",
    "        transformed_labels = []\n",
    "        with tqdm(total=len(df), desc=loading_label) as pbar:\n",
    "            pool = Pool(processes=n_jobs) \n",
    "            documents = df['text'].values\n",
    "            labels = df['label'].values\n",
    "            arguments = create_arguments(documents, labels, 300, dictionary_keys, tokenizer, max_document_length)\n",
    "            for result in pool.imap_unordered(transform_document_mp, arguments):\n",
    "                pbar.update(len(result[0]))\n",
    "                transformed_documents.extend(result[0])\n",
    "                transformed_labels.extend(result[1])\n",
    "        \n",
    "        self.documents = torch.tensor(transformed_documents).type(torch.int32)\n",
    "        self.labels = torch.tensor(transformed_labels).type(torch.int32)\n",
    "                                   \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        document = self.documents[idx]\n",
    "        label = self.labels[idx]\n",
    "        return document, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(max_document_length: int, tokenizer: LemmaTokenizer, dictionary_keys: list, n_jobs = 10):\n",
    "    \"\"\"\n",
    "    Get the datasets for training, validation and test.\n",
    "    :param max_document_length: the max length of the documents\n",
    "    \"\"\"\n",
    "\n",
    "    # Define Datasets and create Dataloader\n",
    "    train_dataset = DocumentsDataset(training_df, max_document_length, tokenizer, dictionary_keys, n_jobs, 'Loading Training Dataset')\n",
    "    val_dataset = DocumentsDataset(validation_df, max_document_length, tokenizer, dictionary_keys, n_jobs, 'Loading Validation Dataset')\n",
    "    test_dataset = DocumentsDataset(test_df, max_document_length, tokenizer, dictionary_keys, n_jobs, 'Loading Test Dataset')\n",
    "\n",
    "    return (train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b147af08e1f34e3a87af62dd8be5c146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Training Dataset:   0%|          | 0/26600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127c21253ea140a781023188706f95f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Validation Dataset:   0%|          | 0/5700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b15dde30034ce7bffe2b57d09e9891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Test Dataset:   0%|          | 0/5700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 26600\n",
      "Validation dataset size: 5700\n",
      "Test dataset size: 5700\n"
     ]
    }
   ],
   "source": [
    "def show_dataset_loading():\n",
    "    train, val, test = get_datasets(max_document_length = 100, tokenizer = LemmaTokenizer(), dictionary_keys = dictionary_keys, n_jobs = 10)\n",
    "    print('Training dataset size: {}'.format(len(train)))\n",
    "    print('Validation dataset size: {}'.format(len(val)))\n",
    "    print('Test dataset size: {}'.format(len(test)))\n",
    "\n",
    "show_dataset_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataloader\n",
    "\n",
    "We implement batching using pytorch's dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for the dataloader.\n",
    "    :param batch: the batch\n",
    "    \"\"\"\n",
    "    documents_stacked = torch.zeros(len(batch), len(batch[0][0]), dtype=torch.long)\n",
    "    labels_stacked = torch.zeros(len(batch), dtype=torch.long)\n",
    "    for idx, elem in enumerate(batch):\n",
    "        documents_stacked[idx] = batch[idx][0]\n",
    "        labels_stacked[idx] = batch[idx][1]\n",
    "    return documents_stacked, labels_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size: int, max_document_length: int, tokenizer: LemmaTokenizer, dictionary_keys: list, n_jobs = 10):\n",
    "    \"\"\"\n",
    "    Get the dataloaders for training, validation and test.\n",
    "    :param batch_size: the batch size\n",
    "    :param max_document_length: the max length of the documents\n",
    "    :param tokenizer: the tokenizer\n",
    "    :param dictionary_keys: the dictionary keys\n",
    "    :param n_jobs: the number of jobs for the multiprocessing of the datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset, test_dataset, val_dataset = get_datasets(max_document_length, tokenizer, dictionary_keys, n_jobs)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 2, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 2, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 2, collate_fn=collate_fn)\n",
    "\n",
    "    return (train_dataloader, test_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Optimization, Early Stopping, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Params\n",
    "\n",
    "We create a class to contain all the hyperparams.\n",
    "\n",
    "Since all the code in the previous steps is wrapped in functions and parameterized, it allows us to easily tune parameters in these steps, without having to re-run the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    \n",
    "    def __init__(self, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: torch.device, \n",
    "        loss_function, \n",
    "        num_epochs: int,\n",
    "        early_stopping_patience: int, \n",
    "        batch_size: int,\n",
    "        max_document_length: int,\n",
    "        cut_off_threshold: float,\n",
    "        tokenizer: LemmaTokenizer,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float = 1e-6,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        freeze_weights: bool = False,\n",
    "        random_embedding: bool = False,\n",
    "        bidirectional: bool = False,\n",
    "        n_heads: int = 8,\n",
    "        ):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.loss_function = loss_function\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.batch_size = batch_size\n",
    "        self.max_document_length = max_document_length\n",
    "        self.cut_off_threshold = cut_off_threshold\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.freeze_weights = freeze_weights\n",
    "        self.random_embedding = random_embedding\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_heads = n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_params = {}\n",
    "\n",
    "def process_input(params: Params):\n",
    "    # get the dictionary\n",
    "    using_cache = False\n",
    "    if cached_params.get('cut_off_threshold') is None or cached_params.get('cut_off_threshold') != params.cut_off_threshold:\n",
    "        _, token_dictionary = get_dictionary(params.cut_off_threshold)\n",
    "    else:\n",
    "        print('Using cached dictionary')\n",
    "        using_cache = True\n",
    "        token_dictionary = cached_params.get('token_dictionary')\n",
    "    print(f\"Number of tokens, with cut-off {params.cut_off_threshold}: {len(token_dictionary)}\")\n",
    "\n",
    "    # get the embedding (not really cacheable, since it's being changed in the training process)\n",
    "    word_embedding, dictionary_keys = get_embedding(token_dictionary)\n",
    "    print('word_embedding shape: {}'.format(word_embedding.weight.shape))\n",
    "\n",
    "    # get the data\n",
    "    if cached_params.get('batch_size') != params.batch_size \\\n",
    "        or cached_params.get('max_document_length') != params.max_document_length \\\n",
    "            or cached_params.get('dictionary_keys') != dictionary_keys \\\n",
    "                or using_cache == False:\n",
    "        train_dataloader, validation_dataloader, test_dataloader = get_dataloaders(params.batch_size, params.max_document_length, params.tokenizer, dictionary_keys)\n",
    "    else:\n",
    "        print('Using cached dataloaders')\n",
    "        train_dataloader = cached_params.get('train_dataloader')\n",
    "        validation_dataloader = cached_params.get('validation_dataloader')\n",
    "        test_dataloader = cached_params.get('test_dataloader')\n",
    "    \n",
    "    # set cached values\n",
    "    cached_params['cut_off_threshold'] = params.cut_off_threshold\n",
    "    cached_params['token_dictionary'] = token_dictionary\n",
    "    cached_params['dictionary_keys'] = dictionary_keys\n",
    "    cached_params['batch_size'] = params.batch_size\n",
    "    cached_params['max_document_length'] = params.max_document_length\n",
    "    cached_params['train_dataloader'] = train_dataloader\n",
    "    cached_params['validation_dataloader'] = validation_dataloader\n",
    "    cached_params['test_dataloader'] = test_dataloader\n",
    "    \n",
    "\n",
    "    return (train_dataloader, validation_dataloader, test_dataloader, word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model:torch.nn.Module, dataloader: DataLoader, params: Params):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    for x, y in dataloader:\n",
    "        params.optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(params.device)\n",
    "        y = y.to(params.device)\n",
    "        \n",
    "        y_hat = model.forward(x, device=params.device)\n",
    "        y_hat_idx = torch.argmax(y_hat, axis=1)\n",
    "        accuracy = (torch.sum(y_hat_idx == y)/len(y))\n",
    "\n",
    "        # calculate loss \n",
    "        loss = params.loss_function(y_hat, y)\n",
    "        loss.backward()\n",
    "        params.optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(accuracy.item())\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def eval_model(model:torch.nn.Module, dataloader: DataLoader, params: Params):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(params.device)\n",
    "            y = y.to(params.device)\n",
    "            \n",
    "            y_hat = model.forward(x, device=params.device)\n",
    "            y_hat_idx = torch.argmax(y_hat, axis=1)\n",
    "            accuracy = (torch.sum(y_hat_idx == y)/len(y))\n",
    "\n",
    "            # calculate loss \n",
    "            loss = params.loss_function(y_hat, y)\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accuracies.append(accuracy.item())    \n",
    "    \n",
    "    \n",
    "    return eval_losses, eval_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final analysis of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(array: list, classes: list):\n",
    "    distribution = {}\n",
    "    for i in range(len(classes)):\n",
    "        distribution[classes[i]] = array.count(i)\n",
    "    return distribution\n",
    "\n",
    "def show_performance(model, test_dataloader, params):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(params.device)\n",
    "                y = y.to(params.device)\n",
    "                \n",
    "                y_hat = model.forward(x, device=params.device)\n",
    "                y_hat_idx = torch.argmax(y_hat, axis=1)\n",
    "                y_pred.extend(y_hat_idx.cpu().numpy())\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "\n",
    "    # constant for classes\n",
    "    classes = label_df['label'].values\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # Supress/hide the warning from true_divide\n",
    "    np.seterr(invalid='ignore')\n",
    "    df_cm = pd.DataFrame(np.nan_to_num(cf_matrix/np.sum(cf_matrix, axis=0),0), index = [i for i in classes],\n",
    "                        columns = [i for i in classes])\n",
    "\n",
    "    plt.figure(figsize = (36,7))\n",
    "    ax1 = plt.subplot(1,3,1)\n",
    "    sns.heatmap(df_cm, annot=True, ax=ax1)\n",
    "    ax1.set_title('Confusion matrix (normalized over all predictions)')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "    ax2 = plt.subplot(1,3,2)\n",
    "    dist_1 = get_distribution(y_true, classes)\n",
    "    ax2.bar(dist_1.keys(), dist_1.values())\n",
    "    ax2.set_title('True distribution')\n",
    "\n",
    "    ax3 = plt.subplot(1,3,3)\n",
    "    dist_2 = get_distribution(y_pred, classes)\n",
    "    ax3.bar(dist_2.keys(), dist_2.values())\n",
    "    ax3.set_title('Predicted distribution')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model: torch.nn.Module, train_dataloader:DataLoader, validation_dataloader: DataLoader, test_dataloader: DataLoader, params: Params):\n",
    "    torch.manual_seed(42069)\n",
    "    # used for early stopping\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    patience = params.early_stopping_patience\n",
    "\n",
    "    pbar = tqdm(range(params.num_epochs))\n",
    "    try:\n",
    "        for epoch in pbar:\n",
    "            ### train ###\n",
    "            train_losses, train_accuracies = train_model(model, train_dataloader, params)\n",
    "            \n",
    "            # tensorboard reporting\n",
    "            train_loss = np.mean(train_losses)\n",
    "            writer.add_scalar(tag=\"training/loss\", scalar_value=train_loss, global_step=epoch)\n",
    "            train_accuracy = np.mean(train_accuracies)\n",
    "            writer.add_scalar(tag=\"training/acc\", scalar_value=train_accuracy, global_step=epoch)\n",
    "\n",
    "            ### eval ###\n",
    "            val_losses, val_accuracies = eval_model(model, validation_dataloader, params)\n",
    "\n",
    "            # tensorboard reporting\n",
    "            val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar(tag=\"validation/loss\", scalar_value=val_loss, global_step=epoch)\n",
    "            val_accuracy = np.mean(val_accuracies)\n",
    "            writer.add_scalar(tag=\"validation/acc\", scalar_value=val_accuracy, global_step=epoch)\n",
    "            \n",
    "            # early stopping\n",
    "            if val_accuracy > best_accuracy:\n",
    "                patience = params.early_stopping_patience\n",
    "                best_accuracy = val_accuracy\n",
    "                best_model = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                patience -= 1\n",
    "            \n",
    "            if patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                model.load_state_dict(best_model)\n",
    "                break\n",
    "\n",
    "            # progress bar indication\n",
    "            pbar.set_description(f'Epoch {epoch+1}/{params.num_epochs}')\n",
    "            pbar.set_postfix(train_loss=train_loss, train_accuracy=train_accuracy, val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Manual stopping\")\n",
    "        if(best_model is not None):\n",
    "            model.load_state_dict(best_model)\n",
    "    \n",
    "\n",
    "    show_performance(model, test_dataloader, params)\n",
    "\n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A: Document Classification with Attention (10 points)\n",
    "\n",
    "This task implements a document classification model using Attention networks. The model is called **`ClassificationAttentionModel`** in the code, containing all various variants as explained later. The basic architecture of `ClassificationAttentionModel` is shown in the figure below.\n",
    "\n",
    "<img src=\"attachment:att_model.png\" alt=\"Drawing\" style=\"width: 290px;\"/>\n",
    "\n",
    "The implementation of the basic architecture of `ClassificationAttentionModel` should cover the following points:\n",
    "\n",
    "**Baseline model**  \n",
    "\n",
    "- **Embeddings & RNN Layer (1 points).** The baseline model first fetches the corresponding embeddings of the word IDs of a given batch. Next, a bidirectional LSTM/GRU layer is applied to the word embeddings. The hidden states of this RNN model can be seen as the contextual embeddings of the underlying word embeddings, and will be used in the next layer instead of the original word embeddings.\n",
    "\n",
    "- **Attention Layer (4 points).** To create the Attention layer, implement an attention network that satisfies the needs of the assignment, i.e. masking as explained in the following. While it is suggested that you implement this attention network by yourself from scratch, you are allowed to use any available implementation of (single-head) attention networks. Among various implementations, the only ones that you are **NOT allowed** to use in this task are the ones based on multi-heads like `torch.nn.MultiheadAttention`. In the baseline architecture, the Attention network must be the (single-head) basic dot-product. Query vector is defined in the model initialization as *a vector of learnable parameters*, whose values will be learned together with the other model's parameters during training. The hidden states of the previous layer are passed as the values (and keys) to the Attention layer, where the Query vector is the query of the Attention. An important point in the forward pass of the Attention layer is setting correct padding masks. These should mask out the attention weights of the padded tokens of each batch. The masks excludes the corresponding vectors of these padded tokens from the calculations in Attention. If the mask is not passed correctly, the padded items will be counted in the Softmax of the Attention layer, and influence other attention weights. A sign that it is working correctly is that final attention weights of the corresponding vectors of the padded tokens are equal to zero. \n",
    "\n",
    "- **Prediction Layer (1 points).** The output of the Attention layer for each batch is the corresponding document embedding, passed to the decoder layer (a linear projection) and softmax to predict the probability distribution of the output classes.\n",
    "\n",
    "- **General model functionality (1 points).** \n",
    "\n",
    "\n",
    "**Model variations:** Implement the **two variations** of the baseline model as explained below. Each variation applies only one change to the baseline architecture, making it possible to study the effect of the change. The code of all variations should be inside `ClassificationAttentionModel`, and executing a variation should be done by simply passing the corresponding parameters of the variation to the model. \n",
    "\n",
    "- **No RNN (1 point).** Run the model without the RNN layer.\n",
    "\n",
    "- **Attention Type (1 point).** Implement another type of single-head Attention other than basic dot-product.\n",
    "\n",
    "**Reporting and discussion (1 point).** Report the evaluation results of the baseline model, as well as the ones for all the variations in a table and also in a plot. Discuss which variation(s) appear to be the most effective. Explain your take.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Document Classification with Transformer (10 points)\n",
    "\n",
    "This task implements a document classification model using Transformer Decoder. This model is called **`ClassificationTransformerModel`** in the code, containing all various variants as explained later. \n",
    "\n",
    "The implementation of the basic architecture of `ClassificationTransformerModel` covers the following points:\n",
    "\n",
    "**Baseline model:** The architecture of baseline `ClassificationTransformerModel`is the exactly the same as the one of `ClassificationAttentionModel`. The only difference is that `ClassificationTransformerModel` replaces the Attention layer with a Transformer Decoder. \n",
    "\n",
    "- **Transformer Decoder (3 points).** Use `torch.nn.TransformerDecoder` class or any other implementation of Transformer Decoder. The baseline model consists of one layer of Transformer Decoder and is defined with default parameters of Transformer Decoder. As before, Query vector is defined separately, and used as the query of the Transformer.\n",
    "\n",
    "- **Model Functionality (1 point).**\n",
    "\n",
    "**Model variations:** Implement the **two variations** of the baseline model as explained below. Each variation applies only one change to the baseline architecture, making it possible to study the effect of the change. The code of all variations should be inside `ClassificationTransformerModel`, and executing a variation should be done by simply passing the corresponding parameters of the variation to the model. \n",
    "\n",
    "- **Transformer Encoder (3 point).** Replace the RNN layer with a Transformer Encoder. Similar to the RNN, the aim of this layer is to provide contextual embeddings of the input word embeddings. \n",
    "\n",
    "- **Number of heads (1 point).** Study the effect of increasing/decreasing the number of heads of the Transformer Decoder.\n",
    "\n",
    "- **Number of layers (1 point).** Study the effect of increasing the layers of the Transformer Decoder.\n",
    "\n",
    "\n",
    "**Reporting and discussion (1 point).** Report the evaluation results of the baseline model, as well as the ones for all the variations in a table and also in a plot. Discuss which variation(s) appear to be the most effective. Explain your take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformerModel(torch.nn.Module):\n",
    "        def __init__(self, embedding: torch.nn.Embedding, hidden_dim: int, n_labels: list, num_layers: int = 1, num_heads: int = 8, bidirectional: bool = True):\n",
    "            super(ClassificationTransformerModel, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.n_labels = n_labels\n",
    "            self.num_layers = num_layers\n",
    "            self.num_heads = num_heads\n",
    "\n",
    "            self.embedding = embedding\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "            self.direction_count = 2 if bidirectional else 1\n",
    "            \n",
    "            self.lstm = LSTM(word2vec.vector_size, hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
    "            self.decoder_layer = TransformerDecoderLayer(d_model=hidden_dim*self.direction_count, nhead=num_heads)\n",
    "            self.transformer_decoder = TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "            self.linear = torch.nn.Linear(self.direction_count*hidden_dim*n_labels,n_labels)\n",
    "            self.softmax = torch.nn.Softmax(dim = 1)\n",
    "            self.query = torch.nn.Parameter(torch.randn(n_labels, 1, hidden_dim*self.direction_count))\n",
    "        \n",
    "        def forward(self, x, device):\n",
    "            batch_size = x.shape[0] # account for variable batch size\n",
    "            sequence_length = x.shape[1] # account for variable sequence length\n",
    "            padding_mask = torch.zeros(batch_size, sequence_length, dtype=torch.bool).to(device)\n",
    "            padding_mask = padding_mask.masked_fill(x == 0, True)\n",
    "            # get embeddings\n",
    "            embeddings = self.embedding(x)            \n",
    "\n",
    "            # initialize hidden state\n",
    "            h_i = torch.zeros(self.direction_count, batch_size, self.hidden_dim, device=device)\n",
    "            a_i = torch.zeros(self.direction_count, batch_size, self.hidden_dim, device=device)\n",
    "            # get all hidden states for the sequences\n",
    "            hidden_states = torch.zeros(sequence_length, batch_size, self.hidden_dim*self.direction_count, device=device) # sequence, batch, hidden_dim*directions\n",
    "            # step through each sequence\n",
    "            for i in range(sequence_length):\n",
    "                _, (h_i, a_i) = self.lstm(embeddings[:,i:i+1,:], (h_i, a_i))\n",
    "                hidden_states[i,:,:] = torch.clone(h_i.view(1, batch_size, self.hidden_dim*self.direction_count))\n",
    "            \n",
    "            tgt = self.query.repeat(1,batch_size,1)\n",
    "            out = self.transformer_decoder(tgt, hidden_states, memory_key_padding_mask=padding_mask)\n",
    "            out = out.view(batch_size, -1)\n",
    "            out = self.linear(out)\n",
    "            out = self.softmax(out)\n",
    "\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationRNNModel(torch.nn.Module):\n",
    "        def __init__(self, embedding: torch.nn.Embedding, hidden_dim: int, n_labels: list, drop_out = 0.0, num_layers: int = 1, freeze_embedding: bool = False, random_init: bool = False, bidirectional: bool = False):\n",
    "            super(ClassificationRNNModel, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.embedding = embedding\n",
    "            self.lstm = LSTM(word2vec.vector_size, hidden_dim, num_layers=1, batch_first=True, bidirectional=False)\n",
    "            self.linear = torch.nn.Linear(hidden_dim*35,n_labels)\n",
    "            self.softmax = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "        def forward(self, x, device):\n",
    "            batch_size = x.shape[0] # account for variable batch size\n",
    "            sequence_length = x.shape[1] # account for variable sequence length\n",
    "            embeddings = self.embedding(x)\n",
    "            # _, (h_n, _) = self.lstm(embeddings)\n",
    "            # h_n = h_n.view(h_n.shape[1], -1)\n",
    "\n",
    "            # initialize hidden state\n",
    "            h_i = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "            a_i = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "            # get all hidden states for the sequences\n",
    "            hidden_states = torch.zeros(sequence_length, batch_size, self.hidden_dim, device=device) # sequence, batch, hidden_dim*directions\n",
    "            # step through each sequence\n",
    "            for i in range(sequence_length):\n",
    "                _, (h_i, a_i) = self.lstm(embeddings[:,i:i+1,:], (h_i, a_i))\n",
    "                hidden_states[i,:,:] = torch.clone(h_i.view(1, batch_size, self.hidden_dim))\n",
    "\n",
    "            # d = self.linear(h_n)\n",
    "            hidden_states = hidden_states.view(batch_size, -1)\n",
    "            d = self.linear(hidden_states)\n",
    "            d = self.softmax(d)\n",
    "            return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached dictionary\n",
      "Number of tokens, with cut-off 1e-06: 22152\n",
      "word_embedding shape: torch.Size([22154, 300])\n",
      "Using cached dataloaders\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453cf69f19fe487f8090ccb64743f4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of the 2D attn_mask is not correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000059?line=41'>42</a>\u001b[0m params\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39moptimizer(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mlearning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000059?line=43'>44</a>\u001b[0m \u001b[39m# train and val\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000059?line=44'>45</a>\u001b[0m best_accuracy \u001b[39m=\u001b[39m train_and_eval(model, train_dataloader, validation_dataloader, test_dataloader, params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000059?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest validation accuracy: \u001b[39m\u001b[39m{\u001b[39;00mbest_accuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb Cell 52'\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_dataloader, validation_dataloader, test_dataloader, params)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=8'>9</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=10'>11</a>\u001b[0m         \u001b[39m### train ###\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=11'>12</a>\u001b[0m         train_losses, train_accuracies \u001b[39m=\u001b[39m train_model(model, train_dataloader, params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=13'>14</a>\u001b[0m         \u001b[39m# tensorboard reporting\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000047?line=14'>15</a>\u001b[0m         train_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_losses)\n",
      "\u001b[1;32m/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb Cell 49'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, params)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000046?line=7'>8</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(params\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000046?line=8'>9</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(params\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000046?line=10'>11</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x, device\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000046?line=11'>12</a>\u001b[0m y_hat_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_hat, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000046?line=12'>13</a>\u001b[0m accuracy \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msum(y_hat_idx \u001b[39m==\u001b[39m y)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(y))\n",
      "\u001b[1;32m/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb Cell 57'\u001b[0m in \u001b[0;36mClassificationTransformerModel.forward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000055?line=36'>37</a>\u001b[0m     hidden_states[i,:,:] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone(h_i\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirection_count))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000055?line=38'>39</a>\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,batch_size,\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000055?line=39'>40</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_decoder(tgt, hidden_states, memory_key_padding_mask\u001b[39m=\u001b[39;49mpadding_mask, memory_mask\u001b[39m=\u001b[39;49mpadding_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000055?line=40'>41</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lukaskurz/university/nlp_dl/nlp_deeplearning_2022/Assignment2_Classification.ipynb#ch0000055?line=41'>42</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py:231\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=227'>228</a>\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=229'>230</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=230'>231</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=231'>232</a>\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=232'>233</a>\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=233'>234</a>\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=235'>236</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=236'>237</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py:367\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=364'>365</a>\u001b[0m tgt \u001b[39m=\u001b[39m tgt \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(tgt2)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=365'>366</a>\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(tgt)\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=366'>367</a>\u001b[0m tgt2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(tgt, memory, memory, attn_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=367'>368</a>\u001b[0m                            key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=368'>369</a>\u001b[0m tgt \u001b[39m=\u001b[39m tgt \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(tgt2)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/transformer.py?line=369'>370</a>\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(tgt)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py:978\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=966'>967</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=967'>968</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=968'>969</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=974'>975</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=975'>976</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight)\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=976'>977</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=977'>978</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=978'>979</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=979'>980</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=980'>981</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=981'>982</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=982'>983</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=983'>984</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m    <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=984'>985</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py:4235\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py?line=4232'>4233</a>\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m   <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py?line=4233'>4234</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(attn_mask\u001b[39m.\u001b[39msize()) \u001b[39m!=\u001b[39m [\u001b[39m1\u001b[39m, query\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), key\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)]:\n\u001b[0;32m-> <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py?line=4234'>4235</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe size of the 2D attn_mask is not correct.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py?line=4235'>4236</a>\u001b[0m \u001b[39melif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m   <a href='file:///home/lukaskurz/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/torch/nn/functional.py?line=4236'>4237</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(attn_mask\u001b[39m.\u001b[39msize()) \u001b[39m!=\u001b[39m [bsz \u001b[39m*\u001b[39m num_heads, query\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), key\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)]:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of the 2D attn_mask is not correct."
     ]
    }
   ],
   "source": [
    "params = Params(\n",
    "        tokenizer=LemmaTokenizer(),\n",
    "        cut_off_threshold = 1e-6,\n",
    "        max_document_length=35,\n",
    "        batch_size=32,\n",
    "        num_epochs=60, \n",
    "        early_stopping_patience=10, \n",
    "        learning_rate=1e-4,\n",
    "        hidden_dim=32,\n",
    "        optimizer=torch.optim.Adam, \n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "        loss_function=torch.nn.NLLLoss(),\n",
    "        n_heads=8,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "    )\n",
    "\n",
    "# params = Params(\n",
    "#         tokenizer=LemmaTokenizer(),\n",
    "#         cut_off_threshold = 1e-6,\n",
    "#         max_document_length=35,\n",
    "#         # max_document_length=20,\n",
    "#         batch_size=32,\n",
    "#         num_epochs=40, \n",
    "#         early_stopping_patience=10, \n",
    "#         learning_rate=1e-4,\n",
    "#         optimizer=torch.optim.Adam, \n",
    "#         device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "#         loss_function=torch.nn.NLLLoss(),\n",
    "#         hidden_dim = 100,\n",
    "#         num_layers = 1,\n",
    "#         dropout = 0.1,\n",
    "#     )\n",
    "\n",
    "train_dataloader, validation_dataloader, test_dataloader, word_embedding = process_input(params)\n",
    "\n",
    "# get the model\n",
    "model = ClassificationTransformerModel(word_embedding.to(params.device), params.hidden_dim, len(label_df), bidirectional=params.bidirectional, num_layers=params.num_layers, num_heads=params.n_heads).to(params.device)\n",
    "# model = ClassificationRNNModel(word_embedding.to(params.device), params.hidden_dim, len(label_df), drop_out=params.dropout, num_layers=params.num_layers).to(params.device)\n",
    "\n",
    "# set optimizer\n",
    "params.optimizer = params.optimizer(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# train and val\n",
    "best_accuracy = train_and_eval(model, train_dataloader, validation_dataloader, test_dataloader, params)\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C: Document Classification with BERT (5 points)\n",
    "\n",
    "This task implements a document classification model using BERT. This model is called **`ClassificationBERTModel`**.\n",
    "\n",
    "**Model Functionality (4 points):** Use the `transformers` library from `huggingface` to load a (small) pre-trained BERT model for document classification. Select a BERT model according to your available resources. The available models can be found [here](https://huggingface.co/models) and [here](https://github.com/google-research/bert). The weights of the BERT model should be fine-tuned using the available training data of the task. Consult the documentation of `huggingface` for a correct implementation of the classifier. \n",
    "\n",
    "**Reporting and discussion (1 point).** Evaluate the model and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: Interpreting Attention Weights (3 extra points)\n",
    "\n",
    "The aim of this task is to provide an interpretation of the functionality of one of the models, by exploring which words have the higher contributions to the prediction results of some specific documents. \n",
    "\n",
    "**Attention weights (2.5 extra points):** Select one of the models trained in the previous tasks as you prefer. Select four documents from test set, such that two are correctly and the other two are incorrectly classified by the model. Next, run the forward pass for each of these documents and extract the corresponding attention weights from the model. Separately for each document, interpret the mechanism of the classifier by looking at the words with the highest attention weights. Do these words – in your opinion – provide important information to the predicted class?\n",
    "\n",
    "**Visualization (0.5 overall extra points):** Visualize the attention weights over document’s words. Some suggested tools and resources:\n",
    "\n",
    "https://github.com/minqi/hnatt\n",
    "\n",
    "https://github.com/jiesutd/Text-Attention-Heatmap-Visualization \n",
    "\n",
    "https://github.com/cbaziotis/neat-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7efd3736c1e74096a4df2b30c2657808d72b4ddfee8e0ddd10378a758c3fecff"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
